{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/paiml/awsbigdata/blob/master/Lesson8_AWS_Big_Data_Case_Studies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy6nhdcaVsCH"
   },
   "source": [
    "# Lesson 8 Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "275I4g3bvxaP"
   },
   "source": [
    "Lesson 8:  Case Studies (60 min)\n",
    "\n",
    "* 8.1 Understand Big Data for Sagemaker (12 min)\n",
    "* 8.2 Learn Sagemaker and EMR Integration (12 min)\n",
    "* 8.3 Learn Serverless Production Big Data Application Development (12 min)\n",
    "* 8.4 Implement Containerization for Big Data (12 min)\n",
    "* 8.5 Implement Spot Instances for Big Data Pipeline (12 min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X8kZHKqie1RM"
   },
   "source": [
    "## 8.1 Understand Big Data for Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAwxrTUcfeko"
   },
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJN-eXYDtpVE"
   },
   "source": [
    "#### [Demo] Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "60kLF_hRfgZd"
   },
   "source": [
    "[Manage Machine Learning Experiments with Search](https://docs.aws.amazon.com/sagemaker/latest/dg/search.html)\n",
    "\n",
    "\n",
    "\n",
    "*   Finding training jobs\n",
    "*   Rank training jobs\n",
    "*   Tracing lineage of a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OVuZQBAg-gHf"
   },
   "source": [
    "### Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MYIx7nJT0z2z"
   },
   "source": [
    "![ground_truth](https://user-images.githubusercontent.com/58792/49688683-9bdba100-faca-11e8-8d93-a55ce6c35a92.png)\n",
    "\n",
    "\n",
    "\n",
    "*   Setup and Manage labeling jobs\n",
    "*   Uses active learning and human labeling\n",
    "*   First 500 objects labeled per month are free\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfvNv4E237k9"
   },
   "source": [
    "#### [Demo] Labeling Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pU-_czP4Blj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nl1K5TVY00Pg"
   },
   "source": [
    "### Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "guMyo-9P014a"
   },
   "source": [
    "![notebooks](https://user-images.githubusercontent.com/58792/49688694-d04f5d00-faca-11e8-9fad-eb63b2534b07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o6nEwQ1tGiOl"
   },
   "source": [
    "#### [Demo] Sagemaker Notebooks\n",
    "\n",
    "*   Create and run Jupyter Notebooks\n",
    "  -  Using Jupyter\n",
    "  -  Using JupyterLab\n",
    "  -  Using the terminal\n",
    "  \n",
    "*   Lifecycle configurations\n",
    "\n",
    "*   Git Repositories\n",
    "  - public repositories can be cloned on Notebook launch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKGPvalK02Yt"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwNsFi6602eK"
   },
   "source": [
    "![training](https://user-images.githubusercontent.com/58792/49688717-05f44600-facb-11e8-8d7f-cf33d272573a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UGQQyDJtUN6D"
   },
   "source": [
    "#### [Demo] Sagemaker Training\n",
    "\n",
    "*   Algorithms\n",
    "  -  Create algorithm\n",
    "  -  Subscribe [AWS Marketplace](https://aws.amazon.com/marketplace/search/results?page=1&filters=fulfillment_options%2Cresource_type&fulfillment_options=SAGEMAKER&resource_type=ALGORITHM)\n",
    "\n",
    "  \n",
    "*   Training Jobs\n",
    "\n",
    "*   HyperParameter Tuning Jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F9YxzTDT04lq"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9OALWADP05aM"
   },
   "source": [
    "![inference](https://user-images.githubusercontent.com/58792/49688735-2fad6d00-facb-11e8-94cb-cba9322e309b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RQqvKpzIp1Sl"
   },
   "source": [
    "#### [Demo] Sagemaker Inference\n",
    "\n",
    "*  Compilation jobs\n",
    "\n",
    "*  Model packages\n",
    "\n",
    "*  Models\n",
    "\n",
    "*  Endpoint configurations\n",
    "\n",
    "*  Endpoints\n",
    "\n",
    "*  Batch transform jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Q52AI5CW3eh"
   },
   "source": [
    "### Built in Sagemaker Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NzsQUYX7W5Xa"
   },
   "source": [
    "Table of [algorithms provided by Amazon Sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)\n",
    "\n",
    "![aws_algorithms](https://user-images.githubusercontent.com/58792/49692597-58595500-fb13-11e8-9db3-e1fe371ac36a.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4mpkoTycugo"
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yc--CN_Kolxo"
   },
   "source": [
    "### Sagemaker Built-in Algorithms---Examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QOi-EXNUoz4O"
   },
   "source": [
    "#### BlazingText\n",
    "\n",
    "\n",
    "* unsupervised learning algorithm for generating **Word2Vec embeddings.**\n",
    "* aws blog post [BlazingText](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-blazingtext-parallelizing-word2vec-on-multiple-cpus-or-gpus/)\n",
    "\n",
    "\n",
    "\n",
    "![BlazingText](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2018/01/18/sagemaker-word2vec-3-1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zCtotY8NpxfW"
   },
   "source": [
    "#### DeepAR Forecasting\n",
    "\n",
    "* supervised learning algorithm for forecasting scalar (that is, one-dimensional) time series using recurrent neural networks (RNN)\n",
    "* [DeepAR Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)\n",
    "\n",
    "![DeepAR](https://docs.aws.amazon.com/sagemaker/latest/dg/images/deepar-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MD7T7q0TmHWZ"
   },
   "source": [
    "### Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zUQJRosYe7Q6"
   },
   "source": [
    "[Demo]\n",
    "\n",
    "* **Built in Sagemaker Algorithms Scale:   **\n",
    "\n",
    "---\n",
    "\n",
    "\"We recommend training k-means on CPU instances. You can train on GPU instances, but should limit GPU training to p*.xlarge instances because only one GPU \n",
    "per instance is used.\"\"\n",
    "\n",
    "\n",
    "* County Census Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_tAhECAQfCPB"
   },
   "source": [
    "## 8.2 Learn Sagemaker and EMR Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6zFHAEo1qeog"
   },
   "source": [
    "![kernel](https://user-images.githubusercontent.com/58792/55046667-982f4400-4fff-11e9-8db1-b70766de3f52.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y-rwz3uZfCPD"
   },
   "source": [
    "### Demo\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1252IBFEmghS"
   },
   "source": [
    "![EMR](https://user-images.githubusercontent.com/58792/55369064-d1f1c600-54a9-11e9-9937-9ded09489c6d.png)\n",
    "\n",
    "* [Sagemaker/Spark/EMR Notebooks](https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgMjuk6kxi2u"
   },
   "source": [
    "## 8.3 Learn Serverless Production Big Data Application Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0NvCIcChKca"
   },
   "source": [
    "[Source Code for Demo](https://github.com/noahgift/awslambda/tree/master/example_src)\n",
    "\n",
    "[Demo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rKXgIaJzsTzC"
   },
   "source": [
    "### Creating Timed Lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CQ45bnlfN14W"
   },
   "source": [
    "Creating Serverless Data Pipeline Producers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "INrZ6YMcwFxp"
   },
   "source": [
    "#### Using AWS Lambda with Cloudwatch Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fo7rh-wkwJGO"
   },
   "source": [
    "Can create [cloudwatch timer](https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/ScheduledEvents.html) to call lambda\n",
    "\n",
    "![cloudwatch event lambda](https://user-images.githubusercontent.com/58792/53612460-4c67b700-3b87-11e9-8fb9-b5d30b77431a.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "awt-CAP5wNOF"
   },
   "source": [
    "#### Using AWS Cloudwatch logging with AWS Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AYCfPjkGwNRB"
   },
   "source": [
    "Using cloudwatch logging is an essential step for Lambda Development\n",
    "\n",
    "![cloudwatch](https://user-images.githubusercontent.com/58792/53612528-9355ac80-3b87-11e9-8473-ab28ba860553.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6fl3b_w6wJJM"
   },
   "source": [
    "#### Using AWS Lambda to populate AWS SQS (Simple Queuing Service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOqVcZTbwLZg"
   },
   "source": [
    "1. *** Create new Lambda with Serverless Wizard***\n",
    "2.  ***cd into lambda and install packages on level up***\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "pip3 install boto3 --target ../\n",
    "pip3 install python-json-logger --target ../\n",
    "```\n",
    "\n",
    "3.  ***Test local***\n",
    "4. *** Deploy***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Xe5mNrQIQcJ"
   },
   "source": [
    "\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Dynamo to SQS\n",
    "\"\"\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "\n",
    "DYNAMODB = boto3.resource('dynamodb')\n",
    "TABLE = \"fang\"\n",
    "QUEUE = \"producer\"\n",
    "SQS = boto3.client(\"sqs\")\n",
    "\n",
    "#SETUP LOGGING\n",
    "import logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "LOG = logging.getLogger()\n",
    "LOG.setLevel(logging.INFO)\n",
    "logHandler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter()\n",
    "logHandler.setFormatter(formatter)\n",
    "LOG.addHandler(logHandler)\n",
    "\n",
    "def scan_table(table):\n",
    "    \"\"\"Scans table and return results\"\"\"\n",
    "    \n",
    "    LOG.info(f\"Scanning Table {table}\")\n",
    "    producer_table = DYNAMODB.Table(table)\n",
    "    response = producer_table.scan()\n",
    "    items = response['Items']\n",
    "    LOG.info(f\"Found {len(items)} Items\")\n",
    "    return items\n",
    "\n",
    "def send_sqs_msg(msg, queue_name, delay=0):\n",
    "    \"\"\"Send SQS Message\n",
    "\n",
    "    Expects an SQS queue_name and msg in a dictionary format.\n",
    "    Returns a response dictionary. \n",
    "    \"\"\"\n",
    "\n",
    "    queue_url = SQS.get_queue_url(QueueName=queue_name)[\"QueueUrl\"]\n",
    "    queue_send_log_msg = \"Send message to queue url: %s, with body: %s\" %\\\n",
    "        (queue_url, msg)\n",
    "    LOG.info(queue_send_log_msg)\n",
    "    json_msg = json.dumps(msg)\n",
    "    response = SQS.send_message(\n",
    "        QueueUrl=queue_url,\n",
    "        MessageBody=json_msg,\n",
    "        DelaySeconds=delay)\n",
    "    queue_send_log_msg_resp = \"Message Response: %s for queue url: %s\" %\\\n",
    "        (response, queue_url) \n",
    "    LOG.info(queue_send_log_msg_resp)\n",
    "    return response\n",
    "\n",
    "def send_emissions(table, queue_name):\n",
    "    \"\"\"Send Emissions\"\"\"\n",
    "    \n",
    "    items = scan_table(table=table)\n",
    "    for item in items:\n",
    "        LOG.info(f\"Sending item {item} to queue: {queue_name}\")\n",
    "        response = send_sqs_msg(item, queue_name=queue_name)\n",
    "        LOG.debug(response)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda entrypoint\n",
    "    \"\"\"\n",
    "\n",
    "    extra_logging = {\"table\": TABLE, \"queue\": QUEUE}\n",
    "    LOG.info(f\"event {event}, context {context}\", extra=extra_logging)\n",
    "    send_emissions(table=TABLE, queue_name=QUEUE)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdB-FUHUJ84S"
   },
   "source": [
    "**Successful Local Test**\n",
    "\n",
    "![test local](https://user-images.githubusercontent.com/58792/53637263-8bbdf400-3bd7-11e9-9840-0cb9851fac6a.png)\n",
    "\n",
    "**Verify Messages in SQS**\n",
    "\n",
    "![**SQS**](https://user-images.githubusercontent.com/58792/53637424-fb33e380-3bd7-11e9-8b68-021704da4ce0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAStK9C0NDd4"
   },
   "source": [
    "***Remote Test Needs Correct Role!!!***\n",
    "\n",
    "![role failure](https://user-images.githubusercontent.com/58792/53638025-c45ecd00-3bd9-11e9-848c-6caedc3d9011.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jyZaUa-NOWYr"
   },
   "source": [
    "#### Wire up Cloudwatch Event Trigger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jf353R-xOcBG"
   },
   "source": [
    "1.  Enable Timed Execution of producer\n",
    "2.  Verify messages flowing into SQS\n",
    "\n",
    "![cloudwatch event trigger](https://user-images.githubusercontent.com/58792/53638200-6979a580-3bda-11e9-94ea-9008bdc9c72a.png)\n",
    "\n",
    "***SQS is populating***\n",
    "\n",
    "![alt text](https://user-images.githubusercontent.com/58792/53638351-cecd9680-3bda-11e9-85bb-f5f4bd4450ad.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_iIu63uesj5R"
   },
   "source": [
    "### Creating Event Driven Lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHqZDYOmsj8G"
   },
   "source": [
    "#### Triggering AWS Lambda with AWS SQS Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nohx7lBy2khs"
   },
   "source": [
    "Lambda can now fire on SQS event\n",
    "\n",
    "![SQS Trigger](https://user-images.githubusercontent.com/58792/53644659-f842ee00-3beb-11e9-8527-96ec12acc5f7.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EzwngIdr2kk2"
   },
   "source": [
    "#### Reading AWS SQS Events from AWS Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8hvf6Vc62ns3"
   },
   "source": [
    "\n",
    "\n",
    "```python\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"Entry Point for Lambda\"\"\"\n",
    "\n",
    "    LOG.info(f\"SURVEYJOB LAMBDA, event {event}, context {context}\")\n",
    "    receipt_handle  = event['Records'][0]['receiptHandle'] #sqs message\n",
    "    #'eventSourceARN': 'arn:aws:sqs:us-east-1:561744971673:producer'\n",
    "    event_source_arn = event['Records'][0]['eventSourceARN']\n",
    "    \n",
    "    names = [] #Captured from Queue\n",
    "    \n",
    "    # Process Queue\n",
    "    for record in event['Records']:\n",
    "        body = json.loads(record['body'])\n",
    "        company_name = body['name']\n",
    "        \n",
    "        #Capture for processing\n",
    "        names.append(company_name)\n",
    "        \n",
    "        extra_logging = {\"body\": body, \"company_name\":company_name}\n",
    "        LOG.info(f\"SQS CONSUMER LAMBDA, splitting sqs arn with value: {event_source_arn}\",extra=extra_logging)\n",
    "        qname = event_source_arn.split(\":\")[-1]\n",
    "        extra_logging[\"queue\"] = qname\n",
    "        LOG.info(f\"Attemping Deleting SQS receiptHandle {receipt_handle} with queue_name {qname}\", extra=extra_logging)\n",
    "        res = delete_sqs_msg(queue_name=qname, receipt_handle=receipt_handle)\n",
    "        LOG.info(f\"Deleted SQS receipt_handle {receipt_handle} with res {res}\", extra=extra_logging)\n",
    "    \n",
    "    # Make Pandas dataframe with wikipedia snippts\n",
    "    LOG.info(f\"Creating dataframe with values: {names}\")\n",
    "    df = names_to_wikipedia(names)\n",
    "    \n",
    "    # Perform Sentiment Analysis\n",
    "    df = apply_sentiment(df)\n",
    "    LOG.info(f\"Sentiment from FANG companies: {df.to_dict()}\")\n",
    "    \n",
    "    # Write result to S3\n",
    "    write_s3(df=df, name=names.pop(), bucket=\"fangsentiment\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckRibEoh2n0q"
   },
   "source": [
    "#### Writing results to AWS S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JDn4JdWsqrwy"
   },
   "source": [
    "write dataframe to AWS S3\n",
    "\n",
    "```python\n",
    "### S3\n",
    "def write_s3(df, name, bucket):\n",
    "    \"\"\"Write S3 Bucket\"\"\"\n",
    "\n",
    "    csv_buffer = StringIO()\n",
    "    df.to_csv(csv_buffer)\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    res = s3_resource.Object(bucket, f'{name}_sentiment.csv').\\\n",
    "        put(Body=csv_buffer.getvalue())\n",
    "    LOG.info(f\"result of write name: {name} to bucket: {bucket} with:\\n {res}\")\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dpIE4sgc2tZi"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "noah:/tmp $ aws s3 cp --recursive s3://fangsentiment/ .                                                                                                \n",
    "download: s3://fangsentiment/netflix_sentiment.csv to ./netflix_sentiment.csv\n",
    "download: s3://fangsentiment/google_sentiment.csv to ./google_sentiment.csv\n",
    "download: s3://fangsentiment/facebook_sentiment.csv to ./facebook_sentiment.csv\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmxGK04TgIef"
   },
   "source": [
    "## 8.4 Implement Containerization for Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ShsOjCQgpTH"
   },
   "source": [
    "[Demo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UkfCS4adhVlA"
   },
   "source": [
    "## 8.5 Implement Spot Instances for Big Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PG5mslHioxxM"
   },
   "source": [
    "**Real Massively Parallel Computer Vision Pipeline**\n",
    "\n",
    "![Spot Pipeline](https://user-images.githubusercontent.com/58792/55369313-ed110580-54aa-11e9-83d5-724611b6f8bd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m4qZ3_nssfGk"
   },
   "source": [
    "[Spot Launcher](https://github.com/noahgift/spot_price_machine_learning/blob/master/spot_launcher.py)\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python\n",
    "\"\"\"Launches a test spot instance\"\"\"\n",
    "\n",
    "import click\n",
    "import boto3\n",
    "import base64\n",
    "\n",
    "from sensible.loginit import logger\n",
    "log = logger(__name__)\n",
    "\n",
    "#Tell Boto3 To Enable Debug Logging\n",
    "#boto3.set_stream_logger(name='botocore')\n",
    "\n",
    "@click.group()\n",
    "def cli():\n",
    "    \"\"\"Spot Launcher\"\"\"\n",
    "\n",
    "\n",
    "def user_data_cmds(duration):\n",
    "    \"\"\"Initial cmds to run, takes duration for halt cmd\"\"\"\n",
    "\n",
    "    cmds = \"\"\"\n",
    "        #cloud-config\n",
    "        runcmd:\n",
    "         - echo \"halt\" | at now + {duration} min\n",
    "    \"\"\".format(duration=duration)\n",
    "    return cmds\n",
    "\n",
    "@cli.command(\"launch\")\n",
    "@click.option('--instance', default=\"r4.large\", help='Instance Type')\n",
    "@click.option('--duration', default=\"55\", help='Duration')\n",
    "@click.option('--keyname', default=\"pragai\", help='Key Name')\n",
    "@click.option('--profile', default=\"arn:aws:iam::561744971673:instance-profile/admin\",\n",
    "                     help='IamInstanceProfile')\n",
    "@click.option('--securitygroup', default=\"sg-61706e07\", help='Key Name')\n",
    "@click.option('--ami', default=\"ami-6df1e514\", help='Key Name')\n",
    "def request_spot_instance(duration, instance, keyname, \n",
    "                            profile, securitygroup, ami):\n",
    "    \"\"\"Request spot instance\"\"\"\n",
    "\n",
    "    #import pdb;pdb.set_trace()\n",
    "    user_data = user_data_cmds(duration)\n",
    "    LaunchSpecifications = {\n",
    "            \"ImageId\": ami,\n",
    "            \"InstanceType\": instance,\n",
    "            \"KeyName\": keyname,\n",
    "            \"IamInstanceProfile\": {\n",
    "                \"Arn\": profile\n",
    "            },\n",
    "            \"UserData\": base64.b64encode(user_data.encode(\"ascii\")).\\\n",
    "                decode('ascii'),\n",
    "            \"BlockDeviceMappings\": [\n",
    "                {\n",
    "                    \"DeviceName\": \"/dev/xvda\",\n",
    "                    \"Ebs\": {\n",
    "                        \"DeleteOnTermination\": True,\n",
    "                        \"VolumeType\": \"gp2\",\n",
    "                        \"VolumeSize\": 8,\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"SecurityGroupIds\": [securitygroup]\n",
    "        }\n",
    "\n",
    "    run_args = {\n",
    "            'SpotPrice'           : \"0.8\",\n",
    "            'Type'                : \"one-time\",\n",
    "            'InstanceCount'       : 1,\n",
    "            'LaunchSpecification' : LaunchSpecifications\n",
    "        }\n",
    "\n",
    "    msg_user_data = \"SPOT REQUEST DATA: %s\" % run_args\n",
    "    log.info(msg_user_data)\n",
    "\n",
    "    client = boto3.client('ec2', \"us-west-2\")\n",
    "    reservation = client.request_spot_instances(**run_args)\n",
    "    return reservation\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cli()\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zzyil6lfnG0d"
   },
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TkQMz_c0hVsU"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "* Spot Launch Demo and Walkthrough on Pricing\n",
    "* Spot Instances EMR\n",
    "* Spot Instances AWS Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvTTQ7vAlZL6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZAwxrTUcfeko",
    "OVuZQBAg-gHf",
    "nl1K5TVY00Pg",
    "iKGPvalK02Yt",
    "F9YxzTDT04lq",
    "1Q52AI5CW3eh",
    "yc--CN_Kolxo",
    "MD7T7q0TmHWZ",
    "_tAhECAQfCPB",
    "QgMjuk6kxi2u",
    "rKXgIaJzsTzC",
    "INrZ6YMcwFxp",
    "awt-CAP5wNOF",
    "6fl3b_w6wJJM",
    "jyZaUa-NOWYr",
    "DHqZDYOmsj8G"
   ],
   "include_colab_link": true,
   "name": "Lesson8-AWS-Big-Data-Case-Studies.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
